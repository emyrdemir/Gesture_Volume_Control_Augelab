# MediaPipe graph that performs hair segmentation with TensorFlow Lite on CPU.
# Used in the example in
# mediapipie/examples/android/src/java/com/mediapipe/apps/hairsegmentationgpu,
# and then ported over for CPU ML and web-side usage.

# Images on GPU coming into and out of the graph.
input_stream: "input_frames_gpu"
output_stream: "output_frames_gpu"

max_queue_size: 100

executor: {
  name: ""
  type: "ApplicationThreadExecutor"
}

# For selfie-mode testing, we flip horizontally here.
node: {
  calculator: "ImageTransformationCalculator"
  input_stream: "IMAGE_GPU:input_frames_gpu"
  output_stream: "IMAGE_GPU:flipped_input_video"
  node_options: {
    [type.googleapis.com/mediapipe.ImageTransformationCalculatorOptions]: {
      flip_horizontally: true
    }
  }
}

# Transforms the input image on GPU to a 512x512 image. To scale the image, by
# default it uses the STRETCH scale mode that maps the entire input image to the
# entire transformed image. As a result, image aspect ratio may be changed and
# objects in the image may be deformed (stretched or squeezed), but the hair
# segmentation model used in this graph is agnostic to that deformation.
node: {
  calculator: "ImageTransformationCalculator"
  input_stream: "IMAGE_GPU:flipped_input_video"
  output_stream: "IMAGE_GPU:transformed_input_video"
  node_options: {
    [type.googleapis.com/mediapipe.ImageTransformationCalculatorOptions]: {
      output_width: 512
      output_height: 512
    }
  }
}

# Caches a mask fed back from the previous round of hair segmentation, and upon
# the arrival of the next input image sends out the cached mask with the
# timestamp replaced by that of the input image, essentially generating a packet
# that carries the previous mask. Note that upon the arrival of the very first
# input image, an empty packet is sent out to jump start the feedback loop.
node {
  calculator: "PreviousLoopbackCalculator"
  input_stream: "MAIN:flipped_input_video"
  input_stream: "LOOP:hair_mask"
  input_stream_info: {
    tag_index: "LOOP"
    back_edge: true
  }
  output_stream: "PREV_LOOP:previous_hair_mask"
}

node {
  calculator: "ImageFrameToGpuBufferCalculator"
  input_stream: "previous_hair_mask"
  output_stream: "previous_hair_mask_gpu"
}

# Embeds the hair mask generated from the previous round of hair segmentation
# as the alpha channel of the current input image.
node {
  calculator: "SetAlphaCalculator"
  input_stream: "IMAGE_GPU:transformed_input_video"
  input_stream: "ALPHA_GPU:previous_hair_mask_gpu"
  output_stream: "IMAGE_GPU:mask_embedded_input_video"
}

# Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
# tensor representing the hair segmentation, which has the same width and height
# as the input image tensor.
node {
  calculator: "TfLiteWebGlInferenceCalculator"
  input_stream: "GPU_BUFFER:mask_embedded_input_video"
  output_stream: "segmentation_tensor"
  node_options: {
    [type.googleapis.com/mediapipe.TfLiteConverterCalculatorOptions] {
      zero_center: false
    }
    [type.googleapis.com/mediapipe.TfLiteInferenceCalculatorOptions] {
      model_path: "hair_segmentation.tflite"
    }
  }
}

# Decodes the segmentation tensor generated by the TensorFlow Lite model into a
# mask of values in [0.f, 1.f], stored in the R channel of a CPU buffer. It also
# takes the mask generated previously as another input to improve the temporal
# consistency.
node {
  calculator: "TfLiteTensorsToSegmentationCalculator"
  input_stream: "TENSORS:segmentation_tensor"
  input_stream: "PREV_MASK:previous_hair_mask"
  output_stream: "MASK:hair_mask"
  node_options: {
    [type.googleapis.com/mediapipe.TfLiteTensorsToSegmentationCalculatorOptions]: {
      tensor_width: 512
      tensor_height: 512
      tensor_channels: 2
      combine_with_previous_ratio: 0.9
      output_layer_index: 1
    }
  }
}

node {
  calculator: "ImageFrameToGpuBufferCalculator"
  input_stream: "hair_mask"
  output_stream: "hair_mask_gpu"
}

# Colors the hair segmentation with the color specified in the option.
node {
  calculator: "RecolorCalculator"
  input_stream: "IMAGE_GPU:flipped_input_video"
  input_stream: "MASK_GPU:hair_mask_gpu"
  output_stream: "IMAGE_GPU:output_frames_gpu"
  node_options: {
    [type.googleapis.com/mediapipe.RecolorCalculatorOptions]: {
      color { r: 0 g: 0 b: 255 }
      mask_channel: RED
    }
  }
}
